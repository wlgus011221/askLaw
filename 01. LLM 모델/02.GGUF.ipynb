{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddff322e-8c39-42ec-9b7b-b602a98a4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4ffe0a-7cd9-4b33-91d3-8dd7f7793681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 폴더 기준 gdownPath 변수 이름의 폴더가 생성되고 허깅페이스의 model_id 이름의 모델이 다운로드 됨\n",
    "gdownPath = \"A.X-4.0-Light\"\n",
    "\n",
    "# 불러올 허깅페이스모델 (model_id 변수에 할당)\n",
    "model_id=\"skt/A.X-4.0-Light\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c75c221-dba7-4bab-b63f-5fd797ddd74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMT18\\.asklaw\\Lib\\site-packages\\huggingface_hub\\file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752f43c46eec4df78a9d53fe92046b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928ca7f58c5444fa918af5ea87075097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A.X_logo_ko_4x3.png:   0%|          | 0.00/183k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e920d75d196d4d0a8d444fee310ed8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b0bf08c7f64a8a9c18c7adcaf0bf94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4890095196745aea2ac4f7deb00590b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/130 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8dece1b0b844ad979b32288d00582d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff4a259c2024407b8cf0a7f818f5bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6213a9c74c2d4dcca3cdb74d683ab9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f18595444a745f786aa4ea7a20daeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd72c1866fe436d877c607704ea8bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A.X_logo.png:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af79fc7ae46b4036bd027dddce6ad705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.60G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c552d1228ae04ad1b72bca39bcda3bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4aa47830194fa995792f43676b528f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6951d4651d546cf9b1d138d2c23d4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48c54f52cd64598828a0a4c6fee1fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SMT18\\\\Desktop\\\\포트폴리오\\\\개인과제 - 법령 문서 질의응답 시스템\\\\code\\\\A.X-4.0-Light'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 폴더 기준 폴더 생성\n",
    "snapshot_download(repo_id=model_id, local_dir=gdownPath,\n",
    "                          local_dir_use_symlinks=False, revision=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6005c3ad-ac14-4467-abb8-f787092d560c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "Updating files:  69% (928/1326)\n",
      "Updating files:  70% (929/1326)\n",
      "Updating files:  71% (942/1326)\n",
      "Updating files:  72% (955/1326)\n",
      "Updating files:  73% (968/1326)\n",
      "Updating files:  74% (982/1326)\n",
      "Updating files:  75% (995/1326)\n",
      "Updating files:  76% (1008/1326)\n",
      "Updating files:  77% (1022/1326)\n",
      "Updating files:  78% (1035/1326)\n",
      "Updating files:  79% (1048/1326)\n",
      "Updating files:  80% (1061/1326)\n",
      "Updating files:  81% (1075/1326)\n",
      "Updating files:  82% (1088/1326)\n",
      "Updating files:  83% (1101/1326)\n",
      "Updating files:  84% (1114/1326)\n",
      "Updating files:  85% (1128/1326)\n",
      "Updating files:  86% (1141/1326)\n",
      "Updating files:  87% (1154/1326)\n",
      "Updating files:  88% (1167/1326)\n",
      "Updating files:  89% (1181/1326)\n",
      "Updating files:  90% (1194/1326)\n",
      "Updating files:  91% (1207/1326)\n",
      "Updating files:  92% (1220/1326)\n",
      "Updating files:  93% (1234/1326)\n",
      "Updating files:  94% (1247/1326)\n",
      "Updating files:  95% (1260/1326)\n",
      "Updating files:  96% (1273/1326)\n",
      "Updating files:  97% (1287/1326)\n",
      "Updating files:  98% (1300/1326)\n",
      "Updating files:  99% (1313/1326)\n",
      "Updating files: 100% (1326/1326)\n",
      "Updating files: 100% (1326/1326), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b2ae748-2194-461a-b5a5-b076ebf1ea77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly\n",
      "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
      "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
      "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
      "Requirement already satisfied: numpy~=1.26.4 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from -r llama.cpp\\./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n",
      "Collecting sentencepiece~=0.2.0 (from -r llama.cpp\\./requirements/requirements-convert_legacy_llama.txt (line 2))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "     ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 991.5/991.5 kB 30.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.45.1 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from -r llama.cpp\\./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.51.3)\n",
      "Collecting gguf>=0.1.0 (from -r llama.cpp\\./requirements/requirements-convert_legacy_llama.txt (line 4))\n",
      "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting protobuf<5.0.0,>=4.21.0 (from -r llama.cpp\\./requirements/requirements-convert_legacy_llama.txt (line 5))\n",
      "  Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: torch~=2.2.1 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from -r llama.cpp\\./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2+cu118)\n",
      "Collecting aiohttp~=3.9.3 (from -r llama.cpp\\./requirements/requirements-tool_bench.txt (line 1))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/aiohttp-3.9.5-cp311-cp311-win_amd64.whl (370 kB)\n",
      "     ---------------------------------------- 0.0/370.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 370.8/370.8 kB 22.5 MB/s eta 0:00:00\n",
      "Collecting pytest~=8.3.3 (from -r llama.cpp\\./requirements/requirements-tool_bench.txt (line 2))\n",
      "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting huggingface_hub~=0.23.2 (from -r llama.cpp\\./requirements/requirements-tool_bench.txt (line 3))\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting matplotlib~=3.10.0 (from -r llama.cpp\\./requirements/requirements-tool_bench.txt (line 4))\n",
      "  Using cached matplotlib-3.10.3-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting openai~=1.55.3 (from -r llama.cpp\\./requirements/requirements-tool_bench.txt (line 6))\n",
      "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pandas~=2.2.3 (from -r llama.cpp\\./requirements/requirements-tool_bench.txt (line 7))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "     ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "     ------------------- ------------------- 5.7/11.6 MB 120.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 11.1/11.6 MB 131.2 MB/s eta 0:00:01\n",
      "     -------------------------------------  11.6/11.6 MB 131.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 11.6/11.6 MB 81.8 MB/s eta 0:00:00\n",
      "Collecting prometheus-client~=0.20.0 (from -r llama.cpp\\./requirements/requirements-tool_bench.txt (line 8))\n",
      "  Downloading prometheus_client-0.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: requests~=2.32.3 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from -r llama.cpp\\./requirements/requirements-tool_bench.txt (line 9)) (2.32.4)\n",
      "Collecting wget~=3.2 (from -r llama.cpp\\./requirements/requirements-tool_bench.txt (line 10))\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting typer~=0.15.1 (from -r llama.cpp\\./requirements/requirements-tool_bench.txt (line 11))\n",
      "  Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting seaborn~=0.13.2 (from -r llama.cpp\\./requirements/requirements-tool_bench.txt (line 12))\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp\\./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.18.0)\n",
      "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting transformers<5.0.0,>=4.45.1 (from -r llama.cpp\\./requirements/requirements-convert_legacy_llama.txt (line 3))\n",
      "  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 40.9/40.9 kB 1.9 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 40.2/40.2 kB 1.9 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.52.2-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 40.2/40.2 kB 1.9 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.52.1-py3-none-any.whl.metadata (38 kB)\n",
      "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
      "INFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n",
      "  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.0/44.0 kB 2.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.4/44.4 kB 2.3 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.4/44.4 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.4/44.4 kB 2.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.4/44.4 kB 2.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.1/44.1 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.5/43.5 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.1/44.1 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp\\./requirements/requirements-convert_legacy_llama.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp\\./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp\\./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.45.1->-r llama.cpp\\./requirements/requirements-convert_legacy_llama.txt (line 3))\n",
      "  Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp\\./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp\\./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from torch~=2.2.1->-r llama.cpp\\./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.14.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from torch~=2.2.1->-r llama.cpp\\./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from torch~=2.2.1->-r llama.cpp\\./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from torch~=2.2.1->-r llama.cpp\\./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from torch~=2.2.1->-r llama.cpp\\./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from aiohttp~=3.9.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from aiohttp~=3.9.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 1)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from aiohttp~=3.9.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 1)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from aiohttp~=3.9.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 1)) (6.6.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from aiohttp~=3.9.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 1)) (1.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from pytest~=8.3.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 2)) (0.4.6)\n",
      "Collecting iniconfig (from pytest~=8.3.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 2))\n",
      "  Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest~=8.3.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 2))\n",
      "  Downloading pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib~=3.10.0->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 4))\n",
      "  Using cached contourpy-1.3.2-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib~=3.10.0->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 4))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib~=3.10.0->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 4))\n",
      "  Downloading fonttools-4.58.5-cp311-cp311-win_amd64.whl.metadata (109 kB)\n",
      "     ---------------------------------------- 0.0/109.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 109.0/109.0 kB 6.2 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib~=3.10.0->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 4))\n",
      "  Using cached kiwisolver-1.4.8-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from matplotlib~=3.10.0->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 4)) (11.0.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib~=3.10.0->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 4))\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from matplotlib~=3.10.0->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from openai~=1.55.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 6)) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai~=1.55.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 6))\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from openai~=1.55.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 6)) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai~=1.55.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 6))\n",
      "  Downloading jiter-0.10.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai~=1.55.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 6))\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "     ---------------------------------------- 0.0/68.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 68.0/68.0 kB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from openai~=1.55.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from pandas~=2.2.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from pandas~=2.2.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from requests~=2.32.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 9)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from requests~=2.32.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from requests~=2.32.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 9)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from requests~=2.32.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 9)) (2025.6.15)\n",
      "Collecting click<8.2,>=8.0.0 (from typer~=0.15.1->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 11))\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer~=0.15.1->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 11))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from typer~=0.15.1->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 11)) (14.0.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from httpx<1,>=0.23.0->openai~=1.55.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.55.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 6)) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai~=1.55.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 6))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai~=1.55.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 6))\n",
      "  Using cached pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai~=1.55.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 6))\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from rich>=10.11.0->typer~=0.15.1->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from rich>=10.11.0->typer~=0.15.1->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 11)) (2.19.2)\n",
      "Requirement already satisfied: propcache>=0.2.1 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 1)) (0.3.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from jinja2->torch~=2.2.1->-r llama.cpp\\./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from sympy->torch~=2.2.1->-r llama.cpp\\./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\smt18\\.asklaw\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r llama.cpp\\./requirements/requirements-tool_bench.txt (line 11)) (0.1.2)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.0 MB 15.9 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.2/10.0 MB 15.9 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.2/10.0 MB 17.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.4/10.0 MB 20.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.1/10.0 MB 23.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.2/10.0 MB 27.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 32.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 32.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 27.9 MB/s eta 0:00:00\n",
      "Downloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 96.2/96.2 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.7/413.7 kB 25.2 MB/s eta 0:00:00\n",
      "Downloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
      "   ---------------------------------------- 0.0/343.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 343.6/343.6 kB 20.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n",
      "   ---------------------------------------- 0.0/402.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 402.8/402.8 kB 26.2 MB/s eta 0:00:00\n",
      "Using cached matplotlib-3.10.3-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "Downloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
      "   ---------------------------------------- 0.0/389.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 389.6/389.6 kB 25.3 MB/s eta 0:00:00\n",
      "Downloading prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n",
      "   ---------------------------------------- 0.0/54.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 54.5/54.5 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.4-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.3/45.3 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 294.9/294.9 kB 17.8 MB/s eta 0:00:00\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached contourpy-1.3.2-cp311-cp311-win_amd64.whl (222 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading fonttools-4.58.5-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 72.2 MB/s eta 0:00:00\n",
      "Downloading jiter-0.10.0-cp311-cp311-win_amd64.whl (209 kB)\n",
      "   ---------------------------------------- 0.0/209.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 209.2/209.2 kB 12.4 MB/s eta 0:00:00\n",
      "Using cached kiwisolver-1.4.8-cp311-cp311-win_amd64.whl (71 kB)\n",
      "Downloading pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "   ---------------------------------------- 0.0/444.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 444.8/444.8 kB 27.2 MB/s eta 0:00:00\n",
      "Using cached pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.9/2.4 MB 28.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 30.4 MB/s eta 0:00:00\n",
      "Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (pyproject.toml): started\n",
      "  Building wheel for wget (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9712 sha256=7ed216b63c72ddd88d93e9a80d9c35650e1649f4df9253bc0b5ed8ea8aad69ac\n",
      "  Stored in directory: c:\\users\\smt18\\appdata\\local\\pip\\cache\\wheels\\40\\b3\\0f\\a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
      "Successfully built wget\n",
      "Installing collected packages: wget, sentencepiece, typing-inspection, shellingham, pyparsing, pydantic-core, protobuf, prometheus-client, pluggy, kiwisolver, jiter, iniconfig, fonttools, distro, cycler, contourpy, click, annotated-types, pytest, pydantic, pandas, matplotlib, huggingface_hub, gguf, aiohttp, typer, tokenizers, seaborn, openai, transformers\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.31.1\n",
      "    Uninstalling protobuf-6.31.1:\n",
      "      Successfully uninstalled protobuf-6.31.1\n",
      "  Attempting uninstall: prometheus-client\n",
      "    Found existing installation: prometheus_client 0.22.1\n",
      "    Uninstalling prometheus_client-0.22.1:\n",
      "      Successfully uninstalled prometheus_client-0.22.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.33.2\n",
      "    Uninstalling huggingface-hub-0.33.2:\n",
      "      Successfully uninstalled huggingface-hub-0.33.2\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.12.13\n",
      "    Uninstalling aiohttp-3.12.13:\n",
      "      Successfully uninstalled aiohttp-3.12.13\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "Successfully installed aiohttp-3.9.5 annotated-types-0.7.0 click-8.1.8 contourpy-1.3.2 cycler-0.12.1 distro-1.9.0 fonttools-4.58.5 gguf-0.17.1 huggingface_hub-0.23.5 iniconfig-2.1.0 jiter-0.10.0 kiwisolver-1.4.8 matplotlib-3.10.3 openai-1.55.3 pandas-2.2.3 pluggy-1.6.0 prometheus-client-0.20.0 protobuf-4.25.8 pydantic-2.11.7 pydantic-core-2.33.2 pyparsing-3.2.3 pytest-8.3.5 seaborn-0.13.2 sentencepiece-0.2.0 shellingham-1.5.4 tokenizers-0.20.3 transformers-4.46.3 typer-0.15.4 typing-inspection-0.4.1 wget-3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\SMT18\\.asklaw\\Lib\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\SMT18\\.asklaw\\Lib\\site-packages\\~andas.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\SMT18\\.asklaw\\Lib\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\SMT18\\.asklaw\\Lib\\site-packages\\~iohttp'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\SMT18\\.asklaw\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.5.0 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.23.5 which is incompatible.\n",
      "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.5 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2434870a-64a2-4280-a605-a2bb8511238e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A.X-4.0-Light'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdownPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bc152be-0e02-49f8-8173-4c60c5a72877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: A.X-4.0-Light\n",
      "INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> Q8_0, shape = {3584, 102400}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> Q8_0, shape = {3584, 102400}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 16384\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3584\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 18944\n",
      "INFO:hf-to-gguf:gguf: head count = 28\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 4\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "WARNING:hf-to-gguf:\n",
      "\n",
      "WARNING:hf-to-gguf:**************************************************************************************\n",
      "WARNING:hf-to-gguf:** WARNING: The BPE pre-tokenizer was not recognized!\n",
      "WARNING:hf-to-gguf:**          There are 2 possible reasons for this:\n",
      "WARNING:hf-to-gguf:**          - the model has not been added to convert_hf_to_gguf_update.py yet\n",
      "WARNING:hf-to-gguf:**          - the pre-tokenization config has changed upstream\n",
      "WARNING:hf-to-gguf:**          Check your model files and convert_hf_to_gguf_update.py and update them accordingly.\n",
      "WARNING:hf-to-gguf:** ref:     https://github.com/ggml-org/llama.cpp/pull/6920\n",
      "WARNING:hf-to-gguf:**\n",
      "WARNING:hf-to-gguf:** chkhsh:  b0a6b1c0bd5998ebd9df08611efde34a4ff03faed45ae09c43e6b31ebd4b94cf\n",
      "WARNING:hf-to-gguf:**************************************************************************************\n",
      "WARNING:hf-to-gguf:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SMT18\\Desktop\\포트폴리오\\개인과제 - 법령 문서 질의응답 시스템\\code\\llama.cpp\\convert_hf_to_gguf.py\", line 2721, in set_vocab\n",
      "    self._set_vocab_sentencepiece()\n",
      "  File \"C:\\Users\\SMT18\\Desktop\\포트폴리오\\개인과제 - 법령 문서 질의응답 시스템\\code\\llama.cpp\\convert_hf_to_gguf.py\", line 908, in _set_vocab_sentencepiece\n",
      "    tokens, scores, toktypes = self._create_vocab_sentencepiece()\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SMT18\\Desktop\\포트폴리오\\개인과제 - 법령 문서 질의응답 시스템\\code\\llama.cpp\\convert_hf_to_gguf.py\", line 925, in _create_vocab_sentencepiece\n",
      "    raise FileNotFoundError(f\"File not found: {tokenizer_path}\")\n",
      "FileNotFoundError: File not found: A.X-4.0-Light\\tokenizer.model\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SMT18\\Desktop\\포트폴리오\\개인과제 - 법령 문서 질의응답 시스템\\code\\llama.cpp\\convert_hf_to_gguf.py\", line 6825, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\SMT18\\Desktop\\포트폴리오\\개인과제 - 법령 문서 질의응답 시스템\\code\\llama.cpp\\convert_hf_to_gguf.py\", line 6819, in main\n",
      "    model_instance.write()\n",
      "  File \"C:\\Users\\SMT18\\Desktop\\포트폴리오\\개인과제 - 법령 문서 질의응답 시스템\\code\\llama.cpp\\convert_hf_to_gguf.py\", line 410, in write\n",
      "    self.prepare_metadata(vocab_only=False)\n",
      "  File \"C:\\Users\\SMT18\\Desktop\\포트폴리오\\개인과제 - 법령 문서 질의응답 시스템\\code\\llama.cpp\\convert_hf_to_gguf.py\", line 523, in prepare_metadata\n",
      "    self.set_vocab()\n",
      "  File \"C:\\Users\\SMT18\\Desktop\\포트폴리오\\개인과제 - 법령 문서 질의응답 시스템\\code\\llama.cpp\\convert_hf_to_gguf.py\", line 2723, in set_vocab\n",
      "    self._set_vocab_gpt2()\n",
      "  File \"C:\\Users\\SMT18\\Desktop\\포트폴리오\\개인과제 - 법령 문서 질의응답 시스템\\code\\llama.cpp\\convert_hf_to_gguf.py\", line 844, in _set_vocab_gpt2\n",
      "    tokens, toktypes, tokpre = self.get_vocab_base()\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SMT18\\Desktop\\포트폴리오\\개인과제 - 법령 문서 질의응답 시스템\\code\\llama.cpp\\convert_hf_to_gguf.py\", line 613, in get_vocab_base\n",
      "    tokpre = self.get_vocab_base_pre(tokenizer)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SMT18\\Desktop\\포트폴리오\\개인과제 - 법령 문서 질의응답 시스템\\code\\llama.cpp\\convert_hf_to_gguf.py\", line 832, in get_vocab_base_pre\n",
      "    raise NotImplementedError(\"BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\")\n",
      "NotImplementedError: BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py A.X-4.0-Light --outfile AX4_Light.gguf --outtype q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcdf230b-d5dd-44b0-b5e8-82c0fc98b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트러블슈팅\n",
    "# gguf 변환이 안 되는 상황\n",
    "# 원인 : A.X-4.0-Light 디렉토리 안에 tokenizer.model 파일일 없기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a4a81c8-9ea5-45a5-9a22-da6a1dc8f4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] 지정된 파일을 찾을 수 없습니다: 'llama.cpp'\n",
      "C:\\Users\\SMT18\\Desktop\\포트폴리오\\개인과제 - 법령 문서 질의응답 시스템\\code\\llama.cpp\n",
      "Already up to date.\n",
      "C:\\Users\\SMT18\\Desktop\\포트폴리오\\개인과제 - 법령 문서 질의응답 시스템\\code\n"
     ]
    }
   ],
   "source": [
    "# llama.cpp 최신화\n",
    "%cd llama.cpp\n",
    "!git pull\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "232fc322-9658-4f14-9f35-0572dee961c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0a6b1c0bd5998ebd9df08611efde34a4ff03faed45ae09c43e6b31ebd4b94cf\n"
     ]
    }
   ],
   "source": [
    "# skt/A.X-4.0-Light tokenizer로 해시값을 얻기\n",
    "\n",
    "from hashlib import sha256\n",
    "\n",
    "chktxt = '\\n \\n\\n \\n\\n\\n \\t \\t\\t \\t\\n  \\n   \\n    \\n     \\n🚀 (normal) 😶‍🌫️ (multiple emojis concatenated) ✅ 🦙🦙 3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3 កាន់តែពិសេសអាច😁 ?我想在apple工作1314151天～ ------======= нещо на Български \\'\\'\\'\\'\\'\\'```````\"\"\"\"......!!!!!!?????? I\\'ve been \\'told he\\'s there, \\'RE you sure? \\'M not sure I\\'ll make it, \\'D you like some tea? We\\'Ve a\\'lL'\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 허깅페이스에서 직접 불러오는 경우\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"skt/A.X-4.0-Light\")\n",
    "\n",
    "# 로컬에 있는 모델로 하는 경우\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"A.X-4.0-Light\")\n",
    "chktok = tokenizer.encode(chktxt)\n",
    "chkhsh = sha256(str(chktok).encode()).hexdigest()\n",
    "\n",
    "print(chkhsh)\n",
    "\n",
    "# 출력결과 : b0a6b1c0bd5998ebd9df08611efde34a4ff03faed45ae09c43e6b31ebd4b94cf\n",
    "# 이 결과를 가지고 llama.cpp/convert_hf_gguf.py 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b5192a2-fd52-43d1-9a30-fe93387acb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: A.X-4.0-Light\n",
      "INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> Q8_0, shape = {3584, 102400}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> Q8_0, shape = {3584, 102400}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 16384\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3584\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 18944\n",
      "INFO:hf-to-gguf:gguf: head count = 28\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 4\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 102103 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 0\n",
      "INFO:gguf.vocab:Setting special token type eos to 0\n",
      "INFO:gguf.vocab:Setting add_bos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {%- if tools is iterable and tools | length > 0 %}\n",
      "                    {{- '<|im_start|><|system|>'}}\n",
      "                    {{- '당신은 도구 호출 기능을 갖춘 유용한 도우미입니다. 사용자의 요청을 처리하기 위해서 필요한 도구가 주어진 목록에 있는 경우 도구 호출로 응답하세요.\n",
      "필요한 도구가 목록에 없는 경우에는 도구 호출 없이 사용자가 요구한 정보를 제공하세요.\n",
      "필요한 도구가 목록에 있지만 해당 도구를 호출하는데 필요한 argument 정보가 부족한 경우 해당 정보를 사용자에게 요청하세요.\n",
      "사용자의 요청을 처리하기 위해 여러번 도구를 호출할 수 있어야 합니다.\n",
      "도구 호출 이후 도구 실행 결과를 입력으로 받으면 해당 결과를 활용하여 답변을 생성하세요.\n",
      "\n",
      "다음은 접근할 수 있는 도구들의 목록 입니다:\n",
      "<tools>\n",
      "'}}\n",
      "                    {%- for t in tools %}\n",
      "                        {{- t | tojson }}\n",
      "                        {{- '\n",
      "' }}\n",
      "                    {%- endfor %}\n",
      "                    {{- '</tools>' }}\n",
      "                    {{- '\n",
      "\n",
      "도구를 호출하려면 아래의 JSON으로 응답하세요.\n",
      "도구 호출 형식: <tool_call>{\"name\": 도구 이름, \"arguments\": dictionary 형태의 도구 인자값}</tool_call>' }}\n",
      "                    {{- '<|im_end|>' }}\n",
      "                {%- endif %}\n",
      "        \n",
      "                {%- for message in messages %}\n",
      "                    {%- if message.role == 'system' %}\n",
      "                        {{- '<|im_start|><|system|>' + message.content + '<|im_end|>'}}\n",
      "                    {%- elif message.role == 'user' %}\n",
      "                        {{- '<|im_start|><|user|>' + message.content + '<|im_end|>'}}\n",
      "                    {%- elif message.role == 'assistant' %}\n",
      "                        {{- '<|im_start|><|assistant|>'}}\n",
      "                        {%- set content = '' %}\n",
      "                        {%- if message.content is defined %}\n",
      "                            {%- set content = message.content %}\n",
      "                        {%- endif %}\n",
      "                        \n",
      "                {%- if add_generation_prompt and not (message.reasoning_content is defined and message.reasoning_content is not none) %}\n",
      "                    {%- if '</think>' in message.content %}\n",
      "                        {%- set content = message.content.split('</think>'.strip())[-1].lstrip('\\n') %}\n",
      "                    {%- endif %}\n",
      "                {%- endif %}\n",
      "                \n",
      "                        {{- content}}\n",
      "                        {%- if message.tool_calls is defined %}\n",
      "                            {%- for tool_call in message.tool_calls %}\n",
      "                                {%- if tool_call.function is defined %}\n",
      "                                    {%- set tool_call = tool_call.function %}\n",
      "                                {%- endif %}\n",
      "                                {{- '<tool_call>' }}\n",
      "                                {{- '{' }}\n",
      "                                {{- '\"name\": \"' }}\n",
      "                                {{- tool_call.name }}\n",
      "                                {{- '\"' }}\n",
      "                                {%- if tool_call.arguments is defined %}\n",
      "                                    {{- ', ' }}\n",
      "                                    {{- '\"arguments\": ' }}\n",
      "                                    {{- tool_call.arguments|tojson }}\n",
      "                                {%- endif %}\n",
      "                                {{- '}' }}\n",
      "                                {{- '</tool_call>' }}\n",
      "                            {%- endfor %}\n",
      "                        {%- endif %}\n",
      "                        {{- '<|im_end|>'}}\n",
      "        \n",
      "                    {%- elif message.role == 'tool' %}\n",
      "                        {{- '<|im_start|><|extra_id_13|><tool_output>' + message.content + '</tool_output><|im_end|>'}}\n",
      "                    {%- endif %}\n",
      "                {%- endfor %}\n",
      "                \n",
      "                    {%- if add_generation_prompt %}\n",
      "                        {{- '<|im_start|><|assistant|>' }}\n",
      "                    {%- endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:A.X-4.0-Light.gguf: n_tensors = 339, total_size = 7.7G\n",
      "\n",
      "Writing:   0%|          | 0.00/7.71G [00:00<?, ?byte/s]\n",
      "Writing:   5%|5         | 390M/7.71G [00:08<02:36, 46.9Mbyte/s]\n",
      "Writing:   6%|5         | 462M/7.71G [00:11<03:12, 37.7Mbyte/s]\n",
      "Writing:   7%|6         | 534M/7.71G [00:13<03:02, 39.4Mbyte/s]\n",
      "Writing:   8%|7         | 606M/7.71G [00:14<02:55, 40.6Mbyte/s]\n",
      "Writing:   8%|8         | 622M/7.71G [00:15<02:54, 40.6Mbyte/s]\n",
      "Writing:   8%|8         | 636M/7.71G [00:15<02:52, 41.1Mbyte/s]\n",
      "Writing:   9%|9         | 710M/7.71G [00:18<03:30, 33.4Mbyte/s]\n",
      "Writing:  10%|#         | 782M/7.71G [00:19<03:06, 37.2Mbyte/s]\n",
      "Writing:  11%|#1        | 854M/7.71G [00:21<02:54, 39.4Mbyte/s]\n",
      "Writing:  11%|#1        | 870M/7.71G [00:21<02:53, 39.4Mbyte/s]\n",
      "Writing:  11%|#1        | 883M/7.71G [00:22<02:49, 40.2Mbyte/s]\n",
      "Writing:  12%|#2        | 957M/7.71G [00:25<03:37, 31.1Mbyte/s]\n",
      "Writing:  13%|#3        | 1.03G/7.71G [00:26<03:09, 35.2Mbyte/s]\n",
      "Writing:  14%|#4        | 1.10G/7.71G [00:28<02:54, 38.0Mbyte/s]\n",
      "Writing:  14%|#4        | 1.12G/7.71G [00:28<02:52, 38.3Mbyte/s]\n",
      "Writing:  15%|#4        | 1.13G/7.71G [00:29<02:48, 39.0Mbyte/s]\n",
      "Writing:  16%|#5        | 1.21G/7.71G [00:32<03:28, 31.3Mbyte/s]\n",
      "Writing:  17%|#6        | 1.28G/7.71G [00:33<03:03, 35.1Mbyte/s]\n",
      "Writing:  17%|#7        | 1.35G/7.71G [00:35<02:46, 38.2Mbyte/s]\n",
      "Writing:  18%|#7        | 1.36G/7.71G [00:35<02:45, 38.4Mbyte/s]\n",
      "Writing:  18%|#7        | 1.38G/7.71G [00:35<02:40, 39.4Mbyte/s]\n",
      "Writing:  19%|#8        | 1.45G/7.71G [00:38<03:22, 31.0Mbyte/s]\n",
      "Writing:  20%|#9        | 1.52G/7.71G [00:40<02:54, 35.5Mbyte/s]\n",
      "Writing:  21%|##        | 1.60G/7.71G [00:42<02:37, 38.9Mbyte/s]\n",
      "Writing:  21%|##        | 1.61G/7.71G [00:42<02:35, 39.1Mbyte/s]\n",
      "Writing:  21%|##1       | 1.63G/7.71G [00:42<02:33, 39.7Mbyte/s]\n",
      "Writing:  22%|##2       | 1.70G/7.71G [00:45<03:11, 31.3Mbyte/s]\n",
      "Writing:  23%|##2       | 1.77G/7.71G [00:47<02:45, 35.9Mbyte/s]\n",
      "Writing:  24%|##3       | 1.84G/7.71G [00:48<02:29, 39.2Mbyte/s]\n",
      "Writing:  24%|##4       | 1.86G/7.71G [00:49<02:29, 39.2Mbyte/s]\n",
      "Writing:  24%|##4       | 1.87G/7.71G [00:49<02:26, 39.9Mbyte/s]\n",
      "Writing:  25%|##5       | 1.95G/7.71G [00:52<03:09, 30.5Mbyte/s]\n",
      "Writing:  26%|##6       | 2.02G/7.71G [00:54<02:43, 34.9Mbyte/s]\n",
      "Writing:  27%|##7       | 2.09G/7.71G [00:55<02:25, 38.7Mbyte/s]\n",
      "Writing:  27%|##7       | 2.11G/7.71G [00:56<02:24, 38.9Mbyte/s]\n",
      "Writing:  28%|##7       | 2.12G/7.71G [00:56<02:20, 39.8Mbyte/s]\n",
      "Writing:  28%|##8       | 2.20G/7.71G [01:00<03:28, 26.5Mbyte/s]\n",
      "Writing:  29%|##9       | 2.27G/7.71G [01:01<02:50, 31.9Mbyte/s]\n",
      "Writing:  30%|###       | 2.34G/7.71G [01:03<02:29, 35.9Mbyte/s]\n",
      "Writing:  31%|###       | 2.36G/7.71G [01:03<02:27, 36.4Mbyte/s]\n",
      "Writing:  31%|###       | 2.37G/7.71G [01:04<02:23, 37.2Mbyte/s]\n",
      "Writing:  32%|###1      | 2.44G/7.71G [01:07<03:15, 26.9Mbyte/s]\n",
      "Writing:  33%|###2      | 2.52G/7.71G [01:09<02:42, 32.0Mbyte/s]\n",
      "Writing:  34%|###3      | 2.59G/7.71G [01:10<02:23, 35.8Mbyte/s]\n",
      "Writing:  34%|###3      | 2.60G/7.71G [01:11<02:20, 36.4Mbyte/s]\n",
      "Writing:  34%|###3      | 2.62G/7.71G [01:11<02:15, 37.5Mbyte/s]\n",
      "Writing:  34%|###4      | 2.63G/7.71G [01:11<02:10, 39.0Mbyte/s]\n",
      "Writing:  34%|###4      | 2.65G/7.71G [01:12<02:06, 40.0Mbyte/s]\n",
      "Writing:  35%|###5      | 2.72G/7.71G [01:16<03:43, 22.4Mbyte/s]\n",
      "Writing:  36%|###6      | 2.79G/7.71G [01:18<02:49, 29.1Mbyte/s]\n",
      "Writing:  37%|###7      | 2.87G/7.71G [01:19<02:23, 33.9Mbyte/s]\n",
      "Writing:  37%|###7      | 2.88G/7.71G [01:20<02:19, 34.6Mbyte/s]\n",
      "Writing:  38%|###7      | 2.90G/7.71G [01:20<02:15, 35.5Mbyte/s]\n",
      "Writing:  38%|###8      | 2.97G/7.71G [01:24<03:02, 26.0Mbyte/s]\n",
      "Writing:  39%|###9      | 3.04G/7.71G [01:25<02:27, 31.6Mbyte/s]\n",
      "Writing:  40%|####      | 3.11G/7.71G [01:27<02:08, 35.8Mbyte/s]\n",
      "Writing:  41%|####      | 3.13G/7.71G [01:27<02:06, 36.3Mbyte/s]\n",
      "Writing:  41%|####      | 3.14G/7.71G [01:27<02:02, 37.4Mbyte/s]\n",
      "Writing:  42%|####1     | 3.22G/7.71G [01:31<02:52, 26.1Mbyte/s]\n",
      "Writing:  43%|####2     | 3.29G/7.71G [01:33<02:21, 31.3Mbyte/s]\n",
      "Writing:  44%|####3     | 3.36G/7.71G [01:34<02:02, 35.4Mbyte/s]\n",
      "Writing:  44%|####3     | 3.38G/7.71G [01:35<01:59, 36.2Mbyte/s]\n",
      "Writing:  44%|####3     | 3.39G/7.71G [01:35<01:55, 37.3Mbyte/s]\n",
      "Writing:  45%|####4     | 3.47G/7.71G [01:39<02:39, 26.6Mbyte/s]\n",
      "Writing:  46%|####5     | 3.54G/7.71G [01:40<02:12, 31.5Mbyte/s]\n",
      "Writing:  47%|####6     | 3.61G/7.71G [01:42<01:57, 35.1Mbyte/s]\n",
      "Writing:  47%|####6     | 3.63G/7.71G [01:42<01:54, 35.7Mbyte/s]\n",
      "Writing:  47%|####7     | 3.64G/7.71G [01:43<01:51, 36.6Mbyte/s]\n",
      "Writing:  48%|####8     | 3.71G/7.71G [01:46<02:34, 26.0Mbyte/s]\n",
      "Writing:  49%|####9     | 3.79G/7.71G [01:48<02:05, 31.4Mbyte/s]\n",
      "Writing:  50%|####9     | 3.86G/7.71G [01:49<01:47, 35.8Mbyte/s]\n",
      "Writing:  50%|#####     | 3.87G/7.71G [01:50<01:45, 36.3Mbyte/s]\n",
      "Writing:  50%|#####     | 3.89G/7.71G [01:50<01:42, 37.5Mbyte/s]\n",
      "Writing:  51%|#####1    | 3.96G/7.71G [01:54<02:20, 26.7Mbyte/s]\n",
      "Writing:  52%|#####2    | 4.03G/7.71G [01:55<01:55, 32.0Mbyte/s]\n",
      "Writing:  53%|#####3    | 4.10G/7.71G [01:57<01:39, 36.1Mbyte/s]\n",
      "Writing:  53%|#####3    | 4.12G/7.71G [01:57<01:38, 36.6Mbyte/s]\n",
      "Writing:  54%|#####3    | 4.13G/7.71G [01:58<01:35, 37.6Mbyte/s]\n",
      "Writing:  55%|#####4    | 4.21G/7.71G [02:01<02:14, 26.1Mbyte/s]\n",
      "Writing:  55%|#####5    | 4.28G/7.71G [02:03<01:47, 31.8Mbyte/s]\n",
      "Writing:  56%|#####6    | 4.35G/7.71G [02:04<01:33, 35.9Mbyte/s]\n",
      "Writing:  57%|#####6    | 4.37G/7.71G [02:05<01:32, 36.3Mbyte/s]\n",
      "Writing:  57%|#####6    | 4.38G/7.71G [02:05<01:29, 37.2Mbyte/s]\n",
      "Writing:  58%|#####7    | 4.46G/7.71G [02:09<02:01, 26.8Mbyte/s]\n",
      "Writing:  59%|#####8    | 4.53G/7.71G [02:10<01:38, 32.2Mbyte/s]\n",
      "Writing:  60%|#####9    | 4.60G/7.71G [02:12<01:25, 36.4Mbyte/s]\n",
      "Writing:  60%|#####9    | 4.62G/7.71G [02:12<01:24, 36.9Mbyte/s]\n",
      "Writing:  60%|######    | 4.63G/7.71G [02:13<01:21, 37.7Mbyte/s]\n",
      "Writing:  61%|######    | 4.70G/7.71G [02:16<01:55, 26.0Mbyte/s]\n",
      "Writing:  62%|######1   | 4.78G/7.71G [02:18<01:33, 31.4Mbyte/s]\n",
      "Writing:  63%|######2   | 4.85G/7.71G [02:20<01:21, 35.2Mbyte/s]\n",
      "Writing:  63%|######3   | 4.86G/7.71G [02:20<01:19, 35.9Mbyte/s]\n",
      "Writing:  63%|######3   | 4.88G/7.71G [02:20<01:16, 37.0Mbyte/s]\n",
      "Writing:  64%|######4   | 4.95G/7.71G [02:22<01:07, 40.8Mbyte/s]\n",
      "Writing:  65%|######5   | 5.02G/7.71G [02:24<01:05, 41.2Mbyte/s]\n",
      "Writing:  65%|######5   | 5.04G/7.71G [02:24<01:04, 41.5Mbyte/s]\n",
      "Writing:  65%|######5   | 5.05G/7.71G [02:24<01:02, 42.4Mbyte/s]\n",
      "Writing:  66%|######6   | 5.13G/7.71G [02:28<01:39, 26.1Mbyte/s]\n",
      "Writing:  67%|######7   | 5.20G/7.71G [02:30<01:23, 30.2Mbyte/s]\n",
      "Writing:  68%|######8   | 5.27G/7.71G [02:32<01:13, 33.1Mbyte/s]\n",
      "Writing:  73%|#######3  | 5.66G/7.71G [02:42<00:57, 35.8Mbyte/s]\n",
      "Writing:  74%|#######4  | 5.73G/7.71G [02:46<01:02, 31.6Mbyte/s]\n",
      "Writing:  75%|#######5  | 5.81G/7.71G [02:49<01:05, 29.3Mbyte/s]\n",
      "Writing:  76%|#######6  | 5.88G/7.71G [02:51<00:58, 31.3Mbyte/s]\n",
      "Writing:  77%|#######7  | 5.95G/7.71G [02:53<00:53, 33.1Mbyte/s]\n",
      "Writing:  77%|#######7  | 5.97G/7.71G [02:53<00:52, 33.5Mbyte/s]\n",
      "Writing:  78%|#######7  | 5.98G/7.71G [02:53<00:50, 34.3Mbyte/s]\n",
      "Writing:  78%|#######8  | 6.05G/7.71G [02:57<00:56, 29.4Mbyte/s]\n",
      "Writing:  79%|#######9  | 6.12G/7.71G [02:58<00:49, 32.3Mbyte/s]\n",
      "Writing:  80%|########  | 6.20G/7.71G [03:00<00:43, 34.5Mbyte/s]\n",
      "Writing:  81%|########  | 6.21G/7.71G [03:01<00:42, 35.0Mbyte/s]\n",
      "Writing:  81%|########  | 6.23G/7.71G [03:01<00:41, 35.9Mbyte/s]\n",
      "Writing:  82%|########1 | 6.30G/7.71G [03:04<00:48, 29.2Mbyte/s]\n",
      "Writing:  83%|########2 | 6.37G/7.71G [03:06<00:41, 32.6Mbyte/s]\n",
      "Writing:  84%|########3 | 6.44G/7.71G [03:08<00:36, 35.0Mbyte/s]\n",
      "Writing:  84%|########3 | 6.46G/7.71G [03:08<00:35, 35.5Mbyte/s]\n",
      "Writing:  84%|########3 | 6.47G/7.71G [03:08<00:33, 36.6Mbyte/s]\n",
      "Writing:  85%|########4 | 6.55G/7.71G [03:11<00:39, 29.3Mbyte/s]\n",
      "Writing:  86%|########5 | 6.62G/7.71G [03:13<00:33, 32.5Mbyte/s]\n",
      "Writing:  87%|########6 | 6.69G/7.71G [03:15<00:29, 34.9Mbyte/s]\n",
      "Writing:  87%|########6 | 6.71G/7.71G [03:15<00:28, 35.4Mbyte/s]\n",
      "Writing:  87%|########7 | 6.72G/7.71G [03:16<00:27, 36.4Mbyte/s]\n",
      "Writing:  88%|########8 | 6.80G/7.71G [03:19<00:31, 28.9Mbyte/s]\n",
      "Writing:  89%|########9 | 6.87G/7.71G [03:21<00:25, 32.6Mbyte/s]\n",
      "Writing:  90%|########9 | 6.94G/7.71G [03:22<00:22, 35.0Mbyte/s]\n",
      "Writing:  90%|######### | 6.96G/7.71G [03:23<00:21, 35.4Mbyte/s]\n",
      "Writing:  90%|######### | 6.97G/7.71G [03:23<00:20, 36.3Mbyte/s]\n",
      "Writing:  91%|#########1| 7.04G/7.71G [03:26<00:23, 28.8Mbyte/s]\n",
      "Writing:  92%|#########2| 7.12G/7.71G [03:28<00:18, 32.3Mbyte/s]\n",
      "Writing:  93%|#########3| 7.19G/7.71G [03:30<00:15, 34.2Mbyte/s]\n",
      "Writing:  93%|#########3| 7.20G/7.71G [03:30<00:14, 34.9Mbyte/s]\n",
      "Writing:  94%|#########3| 7.22G/7.71G [03:31<00:13, 35.7Mbyte/s]\n",
      "Writing:  95%|#########4| 7.29G/7.71G [03:35<00:16, 25.0Mbyte/s]\n",
      "Writing:  95%|#########5| 7.36G/7.71G [03:36<00:12, 29.2Mbyte/s]\n",
      "Writing:  96%|#########6| 7.44G/7.71G [03:38<00:08, 32.2Mbyte/s]\n",
      "Writing:  97%|#########6| 7.45G/7.71G [03:39<00:07, 33.2Mbyte/s]\n",
      "Writing:  97%|#########6| 7.46G/7.71G [03:39<00:07, 34.4Mbyte/s]\n",
      "Writing:  98%|#########7| 7.54G/7.71G [03:43<00:07, 24.9Mbyte/s]\n",
      "Writing:  99%|#########8| 7.61G/7.71G [03:45<00:03, 29.2Mbyte/s]\n",
      "Writing: 100%|#########9| 7.68G/7.71G [03:46<00:00, 32.3Mbyte/s]\n",
      "Writing: 100%|#########9| 7.70G/7.71G [03:47<00:00, 33.0Mbyte/s]\n",
      "Writing: 100%|#########9| 7.71G/7.71G [03:47<00:00, 34.3Mbyte/s]\n",
      "Writing: 100%|##########| 7.71G/7.71G [03:48<00:00, 33.8Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to A.X-4.0-Light.gguf\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py A.X-4.0-Light --outfile A.X-4.0-Light.gguf --outtype q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff58bb71-7a39-48b7-b344-96d0e31f1d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스에 gguf 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cb502a6-c941-443c-b711-efa259b906d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3500d91ce86944f3a8f60771adc0763e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import  huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ce8764b-b0f1-4e99-9c15-21c64c83f320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f409ef6fafd7452db15e33f131f9da13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A.X-4.0-Light.gguf:   0%|          | 0.00/7.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/limjh12/A.X-4.0-Light-GGUF/commit/9ba63d245ee7351b731a9783304917af20d3767c', commit_message='Upload A.X-4.0-Light.gguf with huggingface_hub', commit_description='', oid='9ba63d245ee7351b731a9783304917af20d3767c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/limjh12/A.X-4.0-Light-GGUF', endpoint='https://huggingface.co', repo_type='model', repo_id='limjh12/A.X-4.0-Light-GGUF'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "# gguf 파일 경로\n",
    "file_path = \"A.X-4.0-Light.gguf\"\n",
    "\n",
    "# Hugging Face 저장소 이름\n",
    "repo_name = \"limjh12/A.X-4.0-Light-GGUF\"\n",
    "\n",
    "# Hugging Face 저장소 생성\n",
    "api.create_repo(repo_id=repo_name, repo_type=\"model\", private=False)\n",
    "\n",
    "# 파일 업로드\n",
    "api.upload_file(\n",
    "    path_or_fileobj=file_path,\n",
    "    path_in_repo=\"A.X-4.0-Light.gguf\",  # 저장소 내 파일 경로\n",
    "    repo_id=repo_name,\n",
    "    repo_type=\"model\",  # 모델 저장소에 업로드\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef28e0d-677b-4e1a-9a2a-7aa46a0c55c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
